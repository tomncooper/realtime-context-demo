# Quarkus application configuration for SmartShip Query API

# Application name
quarkus.application.name=smartship-query-api

# HTTP configuration
quarkus.http.port=8080
quarkus.http.host=0.0.0.0

# OpenAPI / Swagger UI
quarkus.swagger-ui.always-include=true
quarkus.swagger-ui.path=/swagger-ui

# Logging
quarkus.log.level=INFO
quarkus.log.category."com.smartship".level=DEBUG
quarkus.log.console.format=%d{HH:mm:ss} %-5p [%c{2.}] (%t) %s%e%n

# Container image configuration
quarkus.container-image.group=smartship
quarkus.container-image.name=query-api
quarkus.container-image.tag=latest
quarkus.container-image.builder=jib
quarkus.jib.base-jvm-image=eclipse-temurin:25-jdk-ubi10-minimal

# Kafka Streams Query Service configuration
# Uses headless service for discovering individual stream-processor instances
streams-processor.headless-service=streams-processor-headless.smartship.svc.cluster.local
streams-processor.port=7070

# Cache configuration for instance discovery
quarkus.cache.caffeine.streams-instances.expire-after-write=30S
quarkus.cache.caffeine.streams-instances.maximum-size=10

# PostgreSQL Reactive Client Configuration
quarkus.datasource.db-kind=postgresql
quarkus.datasource.reactive.max-size=10
quarkus.datasource.health.enabled=true

# Production PostgreSQL (K8s deployment)
%prod.quarkus.datasource.username=${POSTGRES_USER:smartship}
%prod.quarkus.datasource.password=${POSTGRES_PASSWORD:smartship123}
%prod.quarkus.datasource.reactive.url=postgresql://${POSTGRES_HOST:postgresql.smartship.svc.cluster.local}:5432/${POSTGRES_DB:smartship}

# Development/Test mode - use DevServices
quarkus.datasource.devservices.enabled=true
quarkus.datasource.devservices.image-name=postgres:15-alpine

# =============================================================================
# Native Image Configuration
# =============================================================================

# Native container build - use UBI 8 Mandrel to match runtime base image
quarkus.native.builder-image=quay.io/quarkus/ubi-quarkus-mandrel-builder-image:jdk-21

# Native container image base (smaller than JVM image)
quarkus.jib.base-native-image=quay.io/quarkus/quarkus-micro-image:2.0

# Native image build optimizations
quarkus.native.resources.includes=META-INF/services/**

# =============================================================================
# LLM Configuration (Phase 6)
# =============================================================================

# Default provider selection: ollama, openai, or anthropic
# Override via LLM_PROVIDER environment variable
quarkus.langchain4j.chat-model.provider=${LLM_PROVIDER:ollama}

# -----------------------------------------------------------------------------
# Ollama Configuration (Primary - Local/Kubernetes)
# -----------------------------------------------------------------------------
quarkus.langchain4j.ollama.base-url=${OLLAMA_BASE_URL:http://ollama.smartship.svc.cluster.local:11434}
quarkus.langchain4j.ollama.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.chat-model.temperature=0.3
quarkus.langchain4j.ollama.timeout=120s
quarkus.langchain4j.ollama.log-requests=true
quarkus.langchain4j.ollama.log-responses=true

# -----------------------------------------------------------------------------
# OpenAI Configuration (Optional - Cloud)
# -----------------------------------------------------------------------------
# Note: Using placeholder default to satisfy config validation when not in use
quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY:not-configured}
quarkus.langchain4j.openai.chat-model.model-name=gpt-4o-mini
quarkus.langchain4j.openai.chat-model.temperature=0.3
quarkus.langchain4j.openai.timeout=60s

# -----------------------------------------------------------------------------
# Anthropic Configuration (Optional - Cloud)
# -----------------------------------------------------------------------------
# Note: Using placeholder default to satisfy config validation when not in use
quarkus.langchain4j.anthropic.api-key=${ANTHROPIC_API_KEY:not-configured}
quarkus.langchain4j.anthropic.chat-model.model-name=claude-3-haiku-20240307
quarkus.langchain4j.anthropic.chat-model.temperature=0.3
quarkus.langchain4j.anthropic.timeout=60s

# -----------------------------------------------------------------------------
# Chat Memory Configuration
# -----------------------------------------------------------------------------
quarkus.langchain4j.chat-memory.memory-window.max-messages=20

# =============================================================================
# Dev Profile - Local Development with Ollama Dev Services
# =============================================================================
%dev.quarkus.langchain4j.ollama.devservices.enabled=true
%dev.quarkus.langchain4j.ollama.devservices.model=llama3.1

# =============================================================================
# Test Profile Configuration (%test)
# =============================================================================

# Mock streams processor for tests (not available during tests)
%test.streams-processor.headless-service=localhost
%test.streams-processor.port=17070

# Reduce logging noise in tests
%test.quarkus.log.category."com.smartship".level=INFO

# Disable Ollama dev services in tests
%test.quarkus.langchain4j.ollama.devservices.enabled=false
